{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain の記法解説 (LCEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:34.489407Z",
     "iopub.status.busy": "2024-06-28T02:32:34.488775Z",
     "iopub.status.idle": "2024-06-28T02:32:34.491583Z",
     "shell.execute_reply": "2024-06-28T02:32:34.491086Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnable と RunnableSequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:12.290335Z",
     "iopub.status.busy": "2024-06-28T02:33:12.290156Z",
     "iopub.status.idle": "2024-06-28T02:33:12.344661Z",
     "shell.execute_reply": "2024-06-28T02:33:12.344241Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザーが入力した料理のレシピを考えてください。\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:12.346689Z",
     "iopub.status.busy": "2024-06-28T02:33:12.346510Z",
     "iopub.status.idle": "2024-06-28T02:33:21.108437Z",
     "shell.execute_reply": "2024-06-28T02:33:21.108007Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_value = prompt.invoke({\"dish\": \"カレー\"})\n",
    "ai_message = model.invoke(prompt_value)\n",
    "output = output_parser.invoke(ai_message)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:21.110332Z",
     "iopub.status.busy": "2024-06-28T02:33:21.110173Z",
     "iopub.status.idle": "2024-06-28T02:33:21.112634Z",
     "shell.execute_reply": "2024-06-28T02:33:21.112238Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:21.114678Z",
     "iopub.status.busy": "2024-06-28T02:33:21.114418Z",
     "iopub.status.idle": "2024-06-28T02:33:26.539851Z",
     "shell.execute_reply": "2024-06-28T02:33:26.539250Z"
    }
   },
   "outputs": [],
   "source": [
    "output = chain.invoke({\"dish\": \"カレー\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnable の実行方法―invoke・stream・batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:26.545129Z",
     "iopub.status.busy": "2024-06-28T02:33:26.544905Z",
     "iopub.status.idle": "2024-06-28T02:33:32.478679Z",
     "shell.execute_reply": "2024-06-28T02:33:32.478134Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "for chunk in chain.stream({\"dish\": \"カレー\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:32.480592Z",
     "iopub.status.busy": "2024-06-28T02:33:32.480406Z",
     "iopub.status.idle": "2024-06-28T02:33:38.976064Z",
     "shell.execute_reply": "2024-06-28T02:33:38.974376Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "outputs = chain.batch([{\"dish\": \"カレー\"}, {\"dish\": \"うどん\"}])\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL の「|」で様々な Runnable を連鎖させる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:38.984366Z",
     "iopub.status.busy": "2024-06-28T02:33:38.983620Z",
     "iopub.status.idle": "2024-06-28T02:33:39.072077Z",
     "shell.execute_reply": "2024-06-28T02:33:39.071585Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:39.073954Z",
     "iopub.status.busy": "2024-06-28T02:33:39.073805Z",
     "iopub.status.idle": "2024-06-28T02:33:39.076746Z",
     "shell.execute_reply": "2024-06-28T02:33:39.076291Z"
    }
   },
   "outputs": [],
   "source": [
    "cot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザーの質問にステップバイステップで回答してください。\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cot_chain = cot_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:39.078597Z",
     "iopub.status.busy": "2024-06-28T02:33:39.078287Z",
     "iopub.status.idle": "2024-06-28T02:33:39.080804Z",
     "shell.execute_reply": "2024-06-28T02:33:39.080464Z"
    }
   },
   "outputs": [],
   "source": [
    "summarize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ステップバイステップで考えた回答から結論だけ抽出してください。\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "summarize_chain = summarize_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:39.082487Z",
     "iopub.status.busy": "2024-06-28T02:33:39.082344Z",
     "iopub.status.idle": "2024-06-28T02:33:42.144944Z",
     "shell.execute_reply": "2024-06-28T02:33:42.144538Z"
    }
   },
   "outputs": [],
   "source": [
    "cot_summarize_chain = cot_chain | summarize_chain\n",
    "ai_message = cot_summarize_chain.invoke({\"question\": \"10 + 2 * 3\"})\n",
    "print(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableLambda―任意の関数を Runnable にする\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:42.146898Z",
     "iopub.status.busy": "2024-06-28T02:33:42.146736Z",
     "iopub.status.idle": "2024-06-28T02:33:42.204154Z",
     "shell.execute_reply": "2024-06-28T02:33:42.203681Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:42.206082Z",
     "iopub.status.busy": "2024-06-28T02:33:42.205909Z",
     "iopub.status.idle": "2024-06-28T02:33:43.030226Z",
     "shell.execute_reply": "2024-06-28T02:33:43.029756Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "chain = prompt | model | output_parser | RunnableLambda(upper)\n",
    "\n",
    "ai_message = chain.invoke({\"input\": \"Hello!\"})\n",
    "print(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chain デコレーターを使った RunnableLamda の実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:43.032211Z",
     "iopub.status.busy": "2024-06-28T02:33:43.032048Z",
     "iopub.status.idle": "2024-06-28T02:33:43.501555Z",
     "shell.execute_reply": "2024-06-28T02:33:43.499283Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "@chain\n",
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "chain = prompt | model | output_parser | upper\n",
    "\n",
    "ai_message = chain.invoke({\"input\": \"Hello!\"})\n",
    "print(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableLambda への自動変換\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:43.508518Z",
     "iopub.status.busy": "2024-06-28T02:33:43.508321Z",
     "iopub.status.idle": "2024-06-28T02:33:43.511080Z",
     "shell.execute_reply": "2024-06-28T02:33:43.510672Z"
    }
   },
   "outputs": [],
   "source": [
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "chain = prompt | model | output_parser | upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:43.512885Z",
     "iopub.status.busy": "2024-06-28T02:33:43.512594Z",
     "iopub.status.idle": "2024-06-28T02:33:43.961318Z",
     "shell.execute_reply": "2024-06-28T02:33:43.960803Z"
    }
   },
   "outputs": [],
   "source": [
    "ai_message = chain.invoke({\"input\": \"Hello!\"})\n",
    "print(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnable の入力の型と出力の型に注意\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:43.963174Z",
     "iopub.status.busy": "2024-06-28T02:33:43.963026Z",
     "iopub.status.idle": "2024-06-28T02:33:43.965674Z",
     "shell.execute_reply": "2024-06-28T02:33:43.965305Z"
    }
   },
   "outputs": [],
   "source": [
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "chain = prompt | model | upper\n",
    "\n",
    "# 以下のコードを実行するとエラーになります\n",
    "# output = chain.invoke({\"input\": \"Hello!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:43.967542Z",
     "iopub.status.busy": "2024-06-28T02:33:43.967246Z",
     "iopub.status.idle": "2024-06-28T02:33:44.531734Z",
     "shell.execute_reply": "2024-06-28T02:33:44.531210Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser() | upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_message = chain.invoke({\"input\": \"Hello!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableParallel―複数の Runnable を並列で処理する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:45.355560Z",
     "iopub.status.busy": "2024-06-28T02:33:45.355402Z",
     "iopub.status.idle": "2024-06-28T02:33:45.407879Z",
     "shell.execute_reply": "2024-06-28T02:33:45.407407Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:45.409670Z",
     "iopub.status.busy": "2024-06-28T02:33:45.409524Z",
     "iopub.status.idle": "2024-06-28T02:33:45.412232Z",
     "shell.execute_reply": "2024-06-28T02:33:45.411889Z"
    }
   },
   "outputs": [],
   "source": [
    "optimistic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"あなたは楽観主義者です。ユーザーの入力に対して楽観的な意見をください。\",\n",
    "        ),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "optimistic_chain = optimistic_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:45.414087Z",
     "iopub.status.busy": "2024-06-28T02:33:45.413807Z",
     "iopub.status.idle": "2024-06-28T02:33:45.416778Z",
     "shell.execute_reply": "2024-06-28T02:33:45.416359Z"
    }
   },
   "outputs": [],
   "source": [
    "pessimistic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"あなたは悲観主義者です。ユーザーの入力に対して悲観的な意見をください。\",\n",
    "        ),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "pessimistic_chain = pessimistic_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:45.418636Z",
     "iopub.status.busy": "2024-06-28T02:33:45.418458Z",
     "iopub.status.idle": "2024-06-28T02:33:48.115947Z",
     "shell.execute_reply": "2024-06-28T02:33:48.115444Z"
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "parallel_chain = RunnableParallel(\n",
    "    {\n",
    "        \"optimistic_opinion\": optimistic_chain,\n",
    "        \"pessimistic_opinion\": pessimistic_chain,\n",
    "    }\n",
    ")\n",
    "\n",
    "ai_message = parallel_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
    "pprint.pprint(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableParallel の出力を Runnable の入力に連結する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:48.117879Z",
     "iopub.status.busy": "2024-06-28T02:33:48.117728Z",
     "iopub.status.idle": "2024-06-28T02:33:54.979992Z",
     "shell.execute_reply": "2024-06-28T02:33:54.978363Z"
    }
   },
   "outputs": [],
   "source": [
    "synthesize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは客観的AIです。2つの意見をまとめてください。\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"楽観的意見: {optimistic_opinion}\\n悲観的意見: {pessimistic_opinion}\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize_chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"optimistic_opinion\": optimistic_chain,\n",
    "            \"pessimistic_opinion\": pessimistic_chain,\n",
    "        }\n",
    "    )\n",
    "    | synthesize_prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "ai_message = synthesize_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
    "print(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableParallel への自動変換\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:54.986651Z",
     "iopub.status.busy": "2024-06-28T02:33:54.986098Z",
     "iopub.status.idle": "2024-06-28T02:33:54.994428Z",
     "shell.execute_reply": "2024-06-28T02:33:54.992735Z"
    }
   },
   "outputs": [],
   "source": [
    "synthesize_chain = (\n",
    "    {\n",
    "        \"optimistic_opinion\": optimistic_chain,\n",
    "        \"pessimistic_opinion\": pessimistic_chain,\n",
    "    }\n",
    "    | synthesize_prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:55.000866Z",
     "iopub.status.busy": "2024-06-28T02:33:55.000312Z",
     "iopub.status.idle": "2024-06-28T02:34:01.170210Z",
     "shell.execute_reply": "2024-06-28T02:34:01.169705Z"
    }
   },
   "outputs": [],
   "source": [
    "ai_message = synthesize_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
    "print(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableLambda との組み合わせ―itemgetter を使う例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:34:01.172136Z",
     "iopub.status.busy": "2024-06-28T02:34:01.171982Z",
     "iopub.status.idle": "2024-06-28T02:34:01.174756Z",
     "shell.execute_reply": "2024-06-28T02:34:01.174370Z"
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "topic_getter = itemgetter(\"topic\")\n",
    "topic = topic_getter({\"topic\": \"生成AIの進化について\"})\n",
    "print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:34:01.176493Z",
     "iopub.status.busy": "2024-06-28T02:34:01.176278Z",
     "iopub.status.idle": "2024-06-28T02:34:09.682638Z",
     "shell.execute_reply": "2024-06-28T02:34:09.682146Z"
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "synthesize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"あなたは客観的AIです。{topic}について2つの意見をまとめてください。\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"楽観的意見: {optimistic_opinion}\\n悲観的意見: {pessimistic_opinion}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "synthesize_chain = (\n",
    "    {\n",
    "        \"optimistic_opinion\": optimistic_chain,\n",
    "        \"pessimistic_opinion\": pessimistic_chain,\n",
    "        \"topic\": itemgetter(\"topic\"),\n",
    "    }\n",
    "    | synthesize_prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "output = synthesize_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnablePassthrough―入力をそのまま出力する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path=\"../tmp/langchain\",\n",
    "    glob=\"**/*.mdx\",\n",
    "    loader_cls=TextLoader,\n",
    ")\n",
    "raw_docs = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(raw_docs)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "db = Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "\n",
    "文脈: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "質問: {question}\n",
    "''')\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "output = chain.invoke(\"LangGraphとは\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assign―RunnableParallel に値を追加する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:34:14.820735Z",
     "iopub.status.busy": "2024-06-28T02:34:14.820536Z",
     "iopub.status.idle": "2024-06-28T02:34:20.477887Z",
     "shell.execute_reply": "2024-06-28T02:34:20.477378Z"
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": retriever,\n",
    "} | RunnablePassthrough.assign(answer=prompt | model | StrOutputParser())\n",
    "\n",
    "output = chain.invoke(\"LangGraphとは\")\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:34:20.479839Z",
     "iopub.status.busy": "2024-06-28T02:34:20.479664Z",
     "iopub.status.idle": "2024-06-28T02:34:27.723756Z",
     "shell.execute_reply": "2024-06-28T02:34:27.723236Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": retriever,\n",
    "    }\n",
    ").assign(answer=prompt | model | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "output = chain.invoke(\"LangGraphとは\")\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （補足）astream_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:34:34.103973Z",
     "iopub.status.busy": "2024-06-28T02:34:34.103802Z",
     "iopub.status.idle": "2024-06-28T02:34:34.107769Z",
     "shell.execute_reply": "2024-06-28T02:34:34.107274Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "async for event in chain.astream_events(\"LangGraphとは\", version=\"v2\"):\n",
    "    print(event, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in chain.astream_events(\"LangGraphとは\", version=\"v2\"):\n",
    "    event_kind = event[\"event\"]\n",
    "\n",
    "    if event_kind == \"on_retriever_end\":\n",
    "        print(\"=== 検索結果 ===\")\n",
    "        documents = event[\"data\"][\"output\"]\n",
    "        for document in documents:\n",
    "            print(document)\n",
    "\n",
    "    elif event_kind == \"on_parser_start\":\n",
    "        print(\"=== 最終出力 ===\")\n",
    "\n",
    "    elif event_kind == \"on_parser_stream\":\n",
    "        chunk = event[\"data\"][\"chunk\"]\n",
    "        print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
